# gather_demo.py
# Module A: The Gatherer (Configurable Edition V3.2 - Fixed Env Loading)

import os
import re
import hashlib
from typing import List, Dict, Any, Optional
from pydantic import BaseModel

try:
    from tavily import TavilyClient
    from rich.console import Console
    from rich.panel import Panel
    from rich.markdown import Markdown
    from rich import box
    from rich.progress import track
    import requests
    from bs4 import BeautifulSoup
    console = Console()
except ImportError:
    print("âŒ ç¼ºå°‘ä¾èµ–åº“ï¼Œè¯·è¿è¡Œ: pip install requests beautifulsoup4 rich tavily-python")
    exit()

# === ğŸ› ï¸ ä¿®å¤æ ¸å¿ƒï¼šæ‰‹åŠ¨åŠ è½½ .env æ–‡ä»¶ ===
def load_env(path=".env"):
    """æ‰‹åŠ¨è¯»å– .env æ–‡ä»¶å¹¶å°†å˜é‡åŠ è½½åˆ°ç¯å¢ƒå˜é‡ä¸­"""
    if not os.path.exists(path):
        console.print(f"[yellow]âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° {path} æ–‡ä»¶ï¼Œè¯·ç¡®è®¤ API Key å·²è®¾ç½®ã€‚[/]")
        return
    
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            # å¿½ç•¥æ³¨é‡Šå’Œç©ºè¡Œ
            if not line or line.startswith("#") or "=" not in line:
                continue
            
            key, value = line.split("=", 1)
            # å»é™¤å¼•å·å’Œç©ºæ ¼
            key = key.strip()
            value = value.strip().strip('"').strip("'")
            
            # è®¾ç½®åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡
            os.environ.setdefault(key, value)

# åœ¨ç¨‹åºå¯åŠ¨æ—¶ç«‹å³æ‰§è¡ŒåŠ è½½
load_env()

# ==========================================

# === é…ç½® ===
WHITELIST = {
    "tier1": {"domains": ["pbc.gov.cn", "mof.gov.cn", "gov.cn"]},
    "tier2": {"domains": ["stcn.com", "caixin.com", "cls.cn", "yicai.com"]}
}

class SourceInfo(BaseModel):
    url: str
    domain: str
    tier: str
    outlet_name: str
    whitelisted: bool

class RawArticle(BaseModel):
    article_id: str
    url: str
    title: str
    snippet: str
    full_text: str = ""
    source: SourceInfo
    eligible_for_event: bool = False

# === æ ¸å¿ƒå‡½æ•° ===

def get_tavily_client():
    key = os.getenv("TAVILY_API_KEY")
    if not key: 
        # å¢åŠ æ›´å‹å¥½çš„æŠ¥é”™æç¤º
        raise ValueError("âŒ æ— æ³•è¯»å– TAVILY_API_KEYã€‚è¯·æ£€æŸ¥é¡¹ç›®æ ¹ç›®å½•ä¸‹æ˜¯å¦æœ‰ .env æ–‡ä»¶ï¼Œä¸”é‡Œé¢å¡«äº† Keyã€‚")
    return TavilyClient(api_key=key)

def extract_body(url: str) -> str:
    """ç®€å•çˆ¬å–æ­£æ–‡"""
    try:
        resp = requests.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
        if resp.status_code != 200: return ""
        soup = BeautifulSoup(resp.text, "html.parser")
        # ç§»é™¤æ‚è´¨
        for tag in soup(["script", "style", "nav", "footer"]): tag.decompose()
        # æ‰¾æ­£æ–‡
        article = soup.find("article") or soup.find("div", class_=re.compile("content|article"))
        return article.get_text(separator="\n", strip=True) if article else soup.body.get_text()[:2000]
    except:
        return ""

def resolve_source(url: str) -> SourceInfo:
    domain = url.split("/")[2].replace("www.", "")
    tier = "unknown"
    whitelisted = False
    
    for t, cfg in WHITELIST.items():
        for d in cfg["domains"]:
            if d in domain:
                tier = t
                whitelisted = True
                break
    
    return SourceInfo(url=url, domain=domain, tier=tier, outlet_name=domain, whitelisted=whitelisted)

def gather(queries: List[str], days: int = 3, max_results: int = 5) -> List[RawArticle]:
    client = get_tavily_client()
    all_articles = []
    
    with console.status(f"[bold green]ğŸ” æ­£åœ¨æœç´¢ (èŒƒå›´: è¿‡å»{days}å¤©)...[/]") as status:
        for query in queries:
            try:
                # è°ƒç”¨ API
                resp = client.search(
                    query=query, 
                    search_depth="advanced", 
                    topic="news",
                    days=days,               
                    max_results=max_results  
                )
                
                for item in resp.get("results", []):
                    source = resolve_source(item["url"])
                    
                    full_text = ""
                    # åªæœ‰ç™½åå•æ‰å»çˆ¬å–å…¨æ–‡ï¼ŒèŠ‚çœæ—¶é—´
                    if source.whitelisted:
                        full_text = extract_body(item["url"])
                    
                    article = RawArticle(
                        article_id=hashlib.md5(item["url"].encode()).hexdigest(),
                        url=item["url"],
                        title=item["title"],
                        snippet=item["content"],
                        full_text=full_text if len(full_text) > 50 else item["content"],
                        source=source,
                        eligible_for_event=True
                    )
                    all_articles.append(article)
                    
            except Exception as e:
                console.print(f"[red]æœç´¢é”™è¯¯: {e}[/]")

    return all_articles

def print_reader_view(articles: List[RawArticle]):
    """é˜…è¯»å™¨æ¨¡å¼å±•ç¤º"""
    console.print("\n")
    if not articles:
        console.print("[yellow]æœªæ‰¾åˆ°ç›¸å…³æ–‡ç« ã€‚[/]")
        return
        
    for i, art in enumerate(articles, 1):
        color = "green" if art.source.whitelisted else "dim"
        console.print(f"[{color}]{i}. {art.title}[/]")
        console.print(f"   [dim]ğŸ”— {art.url}[/dim]")
