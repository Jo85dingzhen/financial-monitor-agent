# gather_demo.py
# Module A: DuckDuckGo News Gatherer
# Compatible with main.py's Financial Monitor Agent

import hashlib
import json
import random
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel

# === ä¾èµ–æ£€æŸ¥ï¼ˆé™é»˜æ¨¡å¼ï¼Œé¿å…è¢« import æ—¶æ‰“å°ï¼‰ ===
try:
    from ddgs import DDGS
except ImportError:
    raise ImportError("è¯·å®‰è£… ddgs: pip install ddgs")

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    raise ImportError("è¯·å®‰è£…: pip install requests beautifulsoup4")

try:
    from rich.console import Console
    from rich.table import Table
    console = Console()
    HAS_RICH = True
except ImportError:
    HAS_RICH = False
    console = None

# === é…ç½® ===
WHITELIST = {
    "tier1": {
        "domains": ["pbc.gov.cn", "mof.gov.cn", "gov.cn", "ndrc.gov.cn", 
                   "stats.gov.cn", "csrc.gov.cn", "nfra.gov.cn", "safe.gov.cn"]
    },
    "tier2": {
        "domains": ["caixin.com", "cls.cn", "yicai.com", "21jingji.com", 
                   "sina.com.cn", "news.cn", "stcn.com", "cs.com.cn", 
                   "cnstock.com", "financialnews.com.cn", "ce.cn", 
                   "jiemian.com", "thepaper.cn", "eeo.com.cn", "nbd.com.cn"]
    }
}

# === æ•°æ®æ¨¡å‹ï¼ˆä¸ main.py å¯¹é½ï¼‰ ===
class SourceInfo(BaseModel):
    url: str
    domain: str
    tier: str
    outlet_name: str  # main.py éœ€è¦è¿™ä¸ªå­—æ®µ
    whitelisted: bool

class RawArticle(BaseModel):
    article_id: str
    url: str
    title: str
    snippet: str
    full_text: str = ""
    source: SourceInfo
    eligible_for_event: bool = False  # main.py å¯èƒ½éœ€è¦
    publish_date: str = ""

# === å·¥å…·å‡½æ•° ===
def _log(msg: str, level: str = "info"):
    """å†…éƒ¨æ—¥å¿—ï¼ˆå¯é€‰ Richï¼‰"""
    if HAS_RICH and console:
        colors = {"info": "cyan", "success": "green", "warning": "yellow", 
                 "error": "red", "debug": "dim"}
        console.print(f"[{colors.get(level, 'white')}]{msg}[/]")
    else:
        print(msg)

def resolve_source(url: str) -> SourceInfo:
    """è§£ææ¥æºä¿¡æ¯"""
    try:
        domain = url.split("/")[2].replace("www.", "")
    except:
        domain = "unknown"
    
    tier = "unknown"
    whitelisted = False
    
    for t, cfg in WHITELIST.items():
        if any(d in domain for d in cfg["domains"]):
            tier = t
            whitelisted = True
            break
    
    return SourceInfo(
        url=url,
        domain=domain,
        tier=tier,
        outlet_name=domain,  # ä¸ main.py å¯¹é½
        whitelisted=whitelisted
    )

# === æœç´¢ç­–ç•¥ ===
def _search_ddgs(query: str, region: str = 'wt-wt', timelimit: Optional[str] = None, 
                fetch_count: int = 30) -> List[Dict]:
    """
    åº•å±‚ DuckDuckGo æœç´¢
    
    Args:
        query: æœç´¢è¯ï¼ˆåŸæ ·ä¼ é€’ï¼Œæ”¯æŒ site:A OR site:B è¯­æ³•ï¼‰
        region: åœ°åŒºä»£ç 
        timelimit: æ—¶é—´é™åˆ¶ ('d'/'w'/'m'/'y'/None)
        fetch_count: å†…éƒ¨æŠ“å–æ¡æ•°ï¼ˆä¸æš´éœ²ç»™å¤–éƒ¨ï¼‰
    """
    try:
        ddgs = DDGS()  # æ¯æ¬¡è°ƒç”¨åˆ›å»ºå®ä¾‹ï¼Œé¿å…çº¿ç¨‹é—®é¢˜
        
        # æ ¹æ®æ˜¯å¦æœ‰æ—¶é—´é™åˆ¶è°ƒç”¨
        if timelimit:
            results = ddgs.text(query, region=region, timelimit=timelimit, max_results=fetch_count)
        else:
            results = ddgs.text(query, region=region, max_results=fetch_count)
        
        if results is None:
            return []
        
        # æ¶ˆè´¹ generator
        return list(results)
        
    except Exception as e:
        _log(f"æœç´¢å¼‚å¸¸: {str(e)[:100]}", "error")
        return []

def _multi_strategy_search(query: str, days: int = 7, fetch_count: int = 30) -> tuple[List[Dict], str]:
    """
    å¤šç­–ç•¥æœç´¢ï¼ˆå¸¦é‡è¯•å’Œå›é€€ï¼‰
    
    Returns:
        (results, strategy_path)
    """
    # æ ¹æ®å¤©æ•°ç¡®å®šæ—¶é—´é™åˆ¶
    if days <= 1:
        timelimit = 'd'
    elif days <= 7:
        timelimit = 'w'
    elif days <= 30:
        timelimit = 'm'
    elif days <= 365:
        timelimit = 'y'
    else:
        timelimit = None
    
    strategy_path = []
    
    # ç­–ç•¥A: å¸¦æ—¶é—´é™åˆ¶
    if timelimit:
        strategy_path.append(f"A(æ—¶é™:{timelimit})")
        for attempt in range(2):  # æŒ‡æ•°é€€é¿é‡è¯•
            results = _search_ddgs(query, region='wt-wt', timelimit=timelimit, fetch_count=fetch_count)
            if results:
                return results, "â†’".join(strategy_path)
            if attempt < 1:
                time.sleep(1 + random.random())
    
    # ç­–ç•¥B: æ— æ—¶é—´é™åˆ¶
    strategy_path.append("B(æ— æ—¶é™)")
    results = _search_ddgs(query, region='wt-wt', timelimit=None, fetch_count=fetch_count)
    if results:
        return results, "â†’".join(strategy_path)
    
    # ç­–ç•¥C: åŒºåŸŸå›é€€
    for region in ['us-en', 'cn-zh']:
        strategy_path.append(f"C({region})")
        results = _search_ddgs(query, region=region, timelimit=None, fetch_count=fetch_count)
        if results:
            return results, "â†’".join(strategy_path)
    
    return [], "â†’".join(strategy_path) + "(å¤±è´¥)"

# === ä¸»é‡‡é›†å‡½æ•°ï¼ˆå¯¹å¤–æ¥å£ï¼‰ ===
def gather(queries: List[str], days: int = 3, max_results: int = 5, 
          save_json: bool = False, output_path: str = "gathered_results.json",
          **kwargs) -> List[RawArticle]:
    """
    ä¸»é‡‡é›†å‡½æ•°ï¼ˆä¸ main.py æ¥å£å¯¹é½ï¼‰
    
    Args:
        queries: æŸ¥è¯¢è¯åˆ—è¡¨ï¼ˆæ”¯æŒ site:A OR site:B è¯­æ³•ï¼‰
        days: æ—¶é—´èŒƒå›´ï¼ˆå¤©æ•°ï¼‰
            - 1 = æœ€è¿‘1å¤©
            - 7 = æœ€è¿‘1å‘¨
            - 30 = æœ€è¿‘1æœˆ
            - 365 = æœ€è¿‘1å¹´
            - 9999 = ä¸é™åˆ¶
        max_results: æ¯ä¸ª query æœ€å¤šè¿”å›å¤šå°‘æ¡"ç™½åå•å‘½ä¸­ä¸”å»é‡å"çš„ RawArticle
        save_json: æ˜¯å¦ä¿å­˜ JSONï¼ˆCLI è°ƒè¯•ç”¨ï¼Œserver æ¨¡å¼å»ºè®®å…³é—­ï¼‰
        output_path: JSON ä¿å­˜è·¯å¾„
        **kwargs: é¢„ç•™æ‰©å±•å‚æ•°ï¼ˆå¦‚ extract_full_text, proxy ç­‰ï¼‰
    
    Returns:
        List[RawArticle]: é‡‡é›†ç»“æœåˆ—è¡¨
    """
    articles = []
    
    # å±€éƒ¨ç»Ÿè®¡ï¼ˆé¿å…å…¨å±€æ±¡æŸ“ï¼‰
    stats = {
        "total_queries": len(queries),
        "total_raw_results": 0,
        "total_filtered": 0,
        "total_hits": 0,
        "strategy_paths": {}
    }
    
    _log(f"ğŸ” å¯åŠ¨é‡‡é›† (æŸ¥è¯¢æ•°: {len(queries)}, æ—¶é—´èŒƒå›´: {days}å¤©, æ¯æŸ¥è¯¢ä¸Šé™: {max_results}æ¡)", "info")
    
    for idx, query in enumerate(queries, 1):
        _log(f"\n[{idx}/{len(queries)}] æŸ¥è¯¢: {query[:60]}...", "info")
        
        # å¤šç­–ç•¥æœç´¢ï¼ˆå†…éƒ¨æŠ“å– 30 æ¡åŸå§‹ç»“æœï¼‰
        results, strategy_path = _multi_strategy_search(query, days=days, fetch_count=30)
        stats["strategy_paths"][query] = strategy_path
        
        if not results:
            _log(f"  âœ— æ— ç»“æœ (ç­–ç•¥: {strategy_path})", "error")
            continue
        
        _log(f"  âœ“ è·å¾— {len(results)} æ¡åŸå§‹ç»“æœ (ç­–ç•¥: {strategy_path})", "success")
        stats["total_raw_results"] += len(results)
        
        found = 0
        filtered = 0
        
        for item in results:
            url = item.get("href", "")
            title = item.get("title", "")
            snippet = item.get("body", "")
            
            if not url or not title:
                continue
            
            source = resolve_source(url)
            
            # ç™½åå•è¿‡æ»¤
            if not source.whitelisted:
                filtered += 1
                if filtered <= 3:  # åªæ˜¾ç¤ºå‰3æ¡
                    _log(f"    - è¿‡æ»¤: {source.domain}", "debug")
                continue
            
            # å»é‡
            if any(a.url == url for a in articles):
                continue
            
            _log(f"    âœ“ å‘½ä¸­: {source.tier} - {source.domain}", "success")
            _log(f"      {title[:60]}...", "debug")
            
            # æ„é€  RawArticle
            article = RawArticle(
                article_id=hashlib.md5(url.encode()).hexdigest(),
                url=url,
                title=title,
                snippet=snippet,
                full_text="",  # å¦‚æœéœ€è¦æå–å…¨æ–‡ï¼Œåœ¨è¿™é‡Œè°ƒç”¨ extract_body()
                source=source,
                eligible_for_event=True,
                publish_date=""  # DuckDuckGo ä¸æä¾›æ—¥æœŸï¼Œä¿æŒä¸ºç©º
            )
            
            articles.append(article)
            found += 1
            
            if found >= max_results:
                break
        
        if filtered > 3:
            _log(f"    ... (è¿‡æ»¤äº†å…¶ä»– {filtered - 3} æ¡)", "debug")
        
        stats["total_filtered"] += filtered
        stats["total_hits"] += found
        
        _log(f"  ğŸ“ˆ æœ¬æŸ¥è¯¢: å‘½ä¸­ {found}, è¿‡æ»¤ {filtered}", "info")
    
    # æ‰“å°ç»Ÿè®¡æ‘˜è¦
    _log(f"\n{'='*60}", "info")
    _log(f"ğŸ“Š é‡‡é›†å®Œæˆ: å…± {len(articles)} æ¡ç»“æœ", "success")
    _log(f"  æ€»æŸ¥è¯¢æ•°: {stats['total_queries']}", "info")
    _log(f"  åŸå§‹ç»“æœæ•°: {stats['total_raw_results']}", "info")
    _log(f"  è¿‡æ»¤ç»“æœæ•°: {stats['total_filtered']}", "info")
    _log(f"  å‘½ä¸­ç™½åå•: {stats['total_hits']}", "info")
    _log(f"{'='*60}", "info")
    
    # å¯é€‰ï¼šä¿å­˜ JSON
    if save_json and articles:
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump([art.model_dump() for art in articles], f, ensure_ascii=False, indent=2)
            _log(f"ğŸ’¾ å·²ä¿å­˜: {output_path}", "success")
        except Exception as e:
            _log(f"âš  ä¿å­˜å¤±è´¥: {e}", "warning")
    
    return articles

# === å¯è§†åŒ–å‡½æ•°ï¼ˆä¾› main.py è°ƒç”¨ï¼‰ ===
def print_reader_view(articles: List[RawArticle]):
    """æ‰“å°é‡‡é›†ç»“æœçš„é˜…è¯»è§†å›¾"""
    if not articles:
        _log("\nâš  æ— é‡‡é›†ç»“æœ", "warning")
        return
    
    if HAS_RICH and console:
        table = Table(title="ğŸ“° é‡‡é›†ç»“æœé¢„è§ˆ", show_lines=True)
        table.add_column("ID", width=4, justify="center")
        table.add_column("æ ‡é¢˜", width=50)
        table.add_column("æ¥æº", width=20)
        table.add_column("å±‚çº§", width=8)
        
        for i, art in enumerate(articles, 1):
            table.add_row(
                str(i),
                art.title[:47] + "..." if len(art.title) > 50 else art.title,
                art.source.domain,
                art.source.tier
            )
        
        console.print("\n")
        console.print(table)
    else:
        print("\n" + "="*60)
        print("ğŸ“° é‡‡é›†ç»“æœé¢„è§ˆ")
        print("="*60)
        for i, art in enumerate(articles, 1):
            print(f"\n[{i}] {art.title}")
            print(f"    æ¥æº: {art.source.domain} ({art.source.tier})")
            print(f"    URL: {art.url}")

# === CLI æµ‹è¯•å…¥å£ï¼ˆä¸ä¼šè¢« import æ—¶æ‰§è¡Œï¼‰ ===
if __name__ == "__main__":
    test_queries = [
        "site:pbc.gov.cn è´§å¸æ”¿ç­–",
        "site:caixin.com é‡‘èç›‘ç®¡"
    ]
    
    articles = gather(
        queries=test_queries,
        days=7,
        max_results=3,
        save_json=True
    )
    
    print_reader_view(articles)