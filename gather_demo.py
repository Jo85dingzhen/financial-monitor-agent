# gatherer_demo.py
# V3.1: Full Whitelist Update & Reader Mode

from __future__ import annotations

import os
import re
import hashlib
import time
from datetime import datetime, timezone
from urllib.parse import urlparse
from typing import Optional, List, Dict, Any

# === ä¾èµ–åº“æ£€æŸ¥ ===
try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦åº“ã€‚è¯·è¿è¡Œ: pip install requests beautifulsoup4")
    exit()

try:
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.text import Text
    from rich.markdown import Markdown
    from rich import box
    from rich.progress import track
    console = Console()
except ImportError:
    print("âŒ é”™è¯¯: ç¼ºå°‘ rich åº“ã€‚è¯·è¿è¡Œ: pip install rich")
    exit()

from pydantic import BaseModel
from tavily import TavilyClient

# =============== 1. ç¯å¢ƒé…ç½® ===============

def load_env(path=".env"):
    if not os.path.exists(path):
        return
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if "=" in line and not line.startswith("#"):
                key, value = line.strip().split("=", 1)
                os.environ.setdefault(key.strip(), value.strip().strip('"').strip("'"))

load_env()

if not os.getenv("TAVILY_API_KEY"):
    console.print("[bold red]âš ï¸  æœªæ‰¾åˆ° TAVILY_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶[/]")

# =============== 2. å®Œæ•´ç™½åå•é…ç½® (å·²æ›´æ–°) ===============

WHITELIST = {
    # Tier 1: æ ¸å¿ƒæ”¿åºœ/ç›‘ç®¡æœºæ„ (9ä¸ª)
    "tier1": {
        "domains": {
            "pbc.gov.cn": "ä¸­å›½äººæ°‘é“¶è¡Œ",
            "mof.gov.cn": "è´¢æ”¿éƒ¨",
            "stats.gov.cn": "å›½å®¶ç»Ÿè®¡å±€",
            "gov.cn": "å›½åŠ¡é™¢/ä¸­å›½æ”¿åºœç½‘",
            "csrc.gov.cn": "è¯ç›‘ä¼š",
            "nfra.gov.cn": "é‡‘èç›‘ç®¡æ€»å±€",
            "safe.gov.cn": "å¤–æ±‡å±€",
            "ndrc.gov.cn": "å›½å®¶å‘æ”¹å§”",
        },
        "max_age_days": 30, # æ”¿ç­–ç±»å…è®¸å›æº¯
    },

    # Tier 2: å®˜æ–¹/å…šåª’/æŒ‡å®šæŠ«éœ²æœºæ„ (21ä¸ª)
    "tier2": {
        "domains": {
            "cs.com.cn": "ä¸­è¯ç½‘/ä¸­å›½è¯åˆ¸æŠ¥",
            "financialnews.com.cn": "é‡‘èæ—¶æŠ¥",
            "financialnews.com": "ä¸­å›½é‡‘èæ–°é—»ç½‘", # åˆ«å
            "stcn.com": "è¯åˆ¸æ—¶æŠ¥",
            "paper.ce.cn": "ç»æµæ—¥æŠ¥(ç”µå­æŠ¥)",
            "ce.cn": "ä¸­å›½ç»æµç½‘",
            "cnstock.com": "ä¸Šè¯æŠ¥",
            "bjnew.com.cn": "æ–°äº¬æŠ¥",
            "jjckb.cn": "ç»æµå‚è€ƒæŠ¥",
            "ceh.com.cn": "ä¸­å›½ç»æµå¯¼æŠ¥", # è¡¥å…¨åŸŸå
            "zhonghongwang.com": "ä¸­å®ç½‘",
            "cfen.com.cn": "ä¸­å›½è´¢ç»æŠ¥ç½‘", # ä¿®æ­£ .com. å†™æ³•
            "chnfund.com": "ä¸­å›½åŸºé‡‘æŠ¥",
            "cet.com.cn": "ä¸­å›½ç»æµæ—¶æŠ¥/æ–°é—»ç½‘",
            "bbtnews.com.cn": "åŒ—äº¬å•†æŠ¥",
            "cbimc.cn": "ä¸­å›½é“¶è¡Œä¿é™©æŠ¥", # ä¿®æ­£ www.
            "eeo.com.cn": "ç»æµè§‚å¯ŸæŠ¥",
            "cb.com.cn": "ä¸­å›½ç»è¥æŠ¥",
            "ccn.com.cn": "ä¸­å›½æ¶ˆè´¹è€…æŠ¥", # è¡¥å…¨åŸŸå
        },
        "max_age_days": 7,
    },

    # Tier 2.5: å¸‚åœºåŒ–æ ¸å¿ƒåª’ä½“ (15ä¸ª)
    "tier2_5": {
        "domains": {
            "caixin.com": "è´¢æ–°",
            "21jingji.com": "21ä¸–çºªç»æµæŠ¥é“",
            "cnfin.com": "æ–°åè´¢ç»",
            "nbd.com.cn": "æ¯æ—¥ç»æµæ–°é—»",
            "yicai.com": "ç¬¬ä¸€è´¢ç»",
            "jwview.com": "ä¸­æ–°ç»çº¬",
            "lanjinger.com": "è“é²¸è´¢ç»",
            "cls.cn": "è´¢è”ç¤¾",
            "sfccn.com": "å—æ–¹è´¢ç»ç½‘",
            "time-weekly.com": "æ—¶ä»£å‘¨æŠ¥",
            "thepaper.cn": "æ¾æ¹ƒæ–°é—»",
            "jiemian.com": "ç•Œé¢æ–°é—»",
            "thecover.cn": "å°é¢æ–°é—»",
            "chinatimes.net.cn": "åå¤æ—¶æŠ¥",
            "shobserver.com": "ä¸Šè§‚æ–°é—»",
        },
        "max_age_days": 3, # å¸‚åœºæ–°é—»æ—¶æ•ˆæ€§è¦æ±‚é«˜
    }
}

# åŸŸååˆ«åæ˜ å°„ (è§£å†³ www. æˆ–ä¸åŒåç¼€æŒ‡å‘åŒä¸€å®¶çš„æƒ…å†µ)
DOMAIN_ALIASES = {
    "www.financialnews.com.cn": "financialnews.com.cn",
    "www.cbimc.cn": "cbimc.cn",
    "paper.ce.cn": "ce.cn", # å½’ç±»åˆ°ä¸­ç»ç½‘ä½“ç³»
}

# =============== 3. æ•°æ®æ¨¡å‹ ===============

class SourceInfo(BaseModel):
    url: str
    domain: str
    tier: Optional[str] = None
    outlet_name: Optional[str] = None
    whitelisted: bool = False

class RawArticle(BaseModel):
    article_id: str
    url: str
    title: str
    snippet: Optional[str] = None
    full_text: Optional[str] = None
    source: SourceInfo
    category: str
    published_at: Optional[str] = None
    eligible_for_event: bool = False
    drop_reason: Optional[str] = None

# =============== 4. æ ¸å¿ƒåŠŸèƒ½ ===============

def get_tavily_client() -> TavilyClient:
    return TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

def tavily_search(query: str, max_results: int = 5) -> List[Dict[str, Any]]:
    client = get_tavily_client()
    try:
        response = client.search(query=query, search_depth="advanced", max_results=max_results)
        return response.get("results", [])
    except Exception as e:
        console.print(f"[red]Tavily æœç´¢å¤±è´¥: {e}[/]")
        return []

def resolve_source(url: str) -> SourceInfo:
    domain = urlparse(url).netloc.lower().replace("www.", "")
    domain = DOMAIN_ALIASES.get(domain, domain)
    
    # éå†ä¸‰å±‚ç™½åå•
    for tier, cfg in WHITELIST.items():
        if domain in cfg["domains"]:
            return SourceInfo(
                url=url, 
                domain=domain, 
                tier=tier, 
                outlet_name=cfg["domains"][domain], 
                whitelisted=True
            )
            
    return SourceInfo(url=url, domain=domain, whitelisted=False)

def extract_article_body(url: str, timeout: int = 15) -> str:
    """ä¸‹è½½å¹¶æ¸…æ´—ç½‘é¡µæ­£æ–‡"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15"
        }
        resp = requests.get(url, headers=headers, timeout=timeout)
        resp.encoding = resp.apparent_encoding
        
        soup = BeautifulSoup(resp.text, "html.parser")
        
        # ç§»é™¤å¹²æ‰°å…ƒç´ 
        for tag in soup(["script", "style", "nav", "header", "footer", "iframe", "noscript", "aside"]):
            tag.decompose()
            
        # æ™ºèƒ½æå–æ­£æ–‡
        article = soup.find("article")
        if not article:
            # å¤‡é€‰æ–¹æ¡ˆï¼šæ‰¾å­—æ•°æœ€å¤šçš„ div
            text_blocks = []
            for div in soup.find_all("div"):
                # ç®€å•è¿‡æ»¤ï¼šç±»ååŒ…å« content, article, body çš„ä¼˜å…ˆ (å¯é€‰ä¼˜åŒ–)
                text = div.get_text(strip=True)
                if len(text) > 150: 
                    text_blocks.append((len(text), div))
            
            if text_blocks:
                text_blocks.sort(key=lambda x: x[0], reverse=True)
                article = text_blocks[0][1]
            else:
                article = soup.body

        if not article: return ""

        text = article.get_text(separator="\n\n")
        return re.sub(r'\n\s*\n', '\n\n', text).strip()

    except Exception as e:
        return f"[Error: {str(e)}]"

# =============== 5. ä¸»æµç¨‹ ===============

def gather(queries: List[str]) -> List[RawArticle]:
    all_results = []
    
    # 1. æœç´¢é˜¶æ®µ
    raw_items = []
    with console.status("[bold green]ğŸ” æ­£åœ¨åŸºäºæ–°ç™½åå•å…¨ç½‘æœç´¢...[/]") as status:
        for q in queries:
            status.update(f"æœç´¢: {q}")
            items = tavily_search(q, max_results=4)
            raw_items.extend(items)
    
    # å»é‡
    seen_urls = set()
    unique_items = []
    for item in raw_items:
        if item['url'] not in seen_urls:
            unique_items.append(item)
            seen_urls.add(item['url'])

    # 2. æŠ“å–ä¸è¿‡æ»¤é˜¶æ®µ
    console.print(f"[cyan]å‘ç° {len(unique_items)} æ¡çº¿ç´¢ï¼Œå¼€å§‹æ·±åº¦è¿‡æ»¤...[/]")
    
    for item in track(unique_items, description="ä¸‹è½½ä¸æ¸…æ´—ä¸­..."):
        url = item["url"]
        source = resolve_source(url)
        
        # ç™½åå•æ£€æŸ¥
        if not source.whitelisted:
            all_results.append(RawArticle(
                article_id="0", url=url, title=item["title"], source=source, 
                category="unknown", content_type="mixed", eligible_for_event=False, drop_reason="éç™½åå•"
            ))
            continue
            
        # å…¨æ–‡ä¸‹è½½
        full_text = extract_article_body(url)
        
        if len(full_text) < 50:
            eligible = False
            drop_reason = "æ­£æ–‡å†…å®¹è¿‡å°‘"
        else:
            eligible = True
            drop_reason = None

        # ç®€å•åˆ†ç±»
        if source.tier == "tier1":
            category = "policy"
        elif "è´¢æŠ¥" in item["title"] or "ä¸šç»©" in item["title"]:
            category = "company"
        else:
            category = "market"
        
        all_results.append(RawArticle(
            article_id=hashlib.md5(url.encode()).hexdigest(),
            url=url, 
            title=item.get("title", ""), 
            snippet=item.get("snippet", ""), 
            full_text=full_text,
            source=source,
            category=category, 
            content_type="fact",
            eligible_for_event=eligible,
            drop_reason=drop_reason
        ))

    return all_results

# =============== 6. ç»“æœå±•ç¤º (Reader View) ===============

def print_reader_view(articles: List[RawArticle]):
    valid_news = [a for a in articles if a.eligible_for_event]
    
    console.print("\n")
    console.rule("[bold cyan]ğŸ“° è´¢ç»æ·±åº¦é˜…è¯»æ¨¡å¼ (V3.1)[/]")
    console.print(f"[dim]ç™½åå•è¦†ç›–: {sum(len(v['domains']) for v in WHITELIST.values())} å®¶æ ¸å¿ƒåª’ä½“[/]", justify="center")
    
    if not valid_news:
        console.print("\n[bold red]âš ï¸ æœ¬æ¬¡æœç´¢æœªå‘½ä¸­ç™½åå•åª’ä½“ã€‚å»ºè®®:[/]")
        console.print("1. æ£€æŸ¥æœç´¢å…³é”®è¯æ˜¯å¦è¿‡äºå†·é—¨")
        console.print("2. å°è¯•æ·»åŠ  'site:domain.com' æŒ‡å®šæœç´¢")
        return

    for i, news in enumerate(valid_news, 1):
        # é¢œè‰²åŒºåˆ† Tier
        color = "red" if news.source.tier == "tier1" else ("blue" if news.source.tier == "tier2" else "green")
        
        console.print(f"\n[bold white on {color}] {i}. {news.title} [/]")
        console.print(f"[dim]æ¥æº: {news.source.outlet_name} ({news.source.tier.upper()}) | å­—æ•°: {len(news.full_text)}[/]")
        console.print(f"[link={news.url}]ğŸ”— åŸæ–‡é“¾æ¥[/link]")
        
        # é¢„è§ˆæ­£æ–‡ (å‰800å­—)
        preview_text = news.full_text[:800] + "\n\n...(å‰©ä½™å†…å®¹çœç•¥)..." if len(news.full_text) > 800 else news.full_text
        
        text_panel = Panel(
            Markdown(preview_text),
            border_style="grey70",
            box=box.SIMPLE,
            title="ğŸ“„ æ­£æ–‡é¢„è§ˆ",
            title_align="left"
        )
        console.print(text_panel)
        console.print("-" * 40, style="dim")

# =============== 7. æ‰§è¡Œå…¥å£ ===============

if __name__ == "__main__":
    # æµ‹è¯•ä¸åŒå±‚çº§çš„åª’ä½“
    QUERIES = [
        "site:pbc.gov.cn è´§å¸æ”¿ç­–",       # Tier 1
        "site:caixin.com å®è§‚æ•°æ®",       # Tier 2.5
        "site:stcn.com ä¸Šå¸‚å…¬å¸",         # Tier 2
        "site:eeo.com.cn ç»æµè§‚å¯Ÿ"        # Tier 2 (æ–°å¢æµ‹è¯•)
    ]
    
    results = gather(QUERIES)
    print_reader_view(results)
