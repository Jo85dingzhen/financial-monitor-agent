# gather_duck_debug.py
# è°ƒè¯•ç‰ˆï¼šè¾“å‡ºæ¯ä¸€æ­¥æ‰§è¡Œæƒ…å†µ

import sys
import time
import hashlib
from typing import List, Dict, Any

print("=" * 60)
print("ğŸš€ å¯åŠ¨ DuckDuckGo é‡‡é›†å™¨ (è°ƒè¯•æ¨¡å¼)")
print("=" * 60)

# === æ­¥éª¤ 1: å¯¼å…¥ä¾èµ– ===
print("\n[1/7] å¯¼å…¥ä¾èµ–...")

try:
    from ddgs import DDGS
    print("  âœ“ ddgs")
except ImportError as e:
    print(f"  âœ— ddgs å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

try:
    from pydantic import BaseModel
    print("  âœ“ pydantic")
except ImportError as e:
    print(f"  âœ— pydantic å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

try:
    import requests
    from bs4 import BeautifulSoup
    print("  âœ“ requests, beautifulsoup4")
except ImportError as e:
    print(f"  âœ— requests/bs4 å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

try:
    from rich.console import Console
    from rich.table import Table
    console = Console()
    HAS_RICH = True
    print("  âœ“ rich")
except ImportError:
    HAS_RICH = False
    console = None
    print("  âš  rich æœªå®‰è£… (å¯é€‰)")

print("\nâœ… æ‰€æœ‰ä¾èµ–å¯¼å…¥æˆåŠŸ")

# === æ­¥éª¤ 2: é…ç½® ===
print("\n[2/7] åŠ è½½é…ç½®...")

WHITELIST = {
    "tier1": {"domains": ["pbc.gov.cn", "mof.gov.cn", "gov.cn", "ndrc.gov.cn"]},
    "tier2": {"domains": ["caixin.com", "cls.cn", "yicai.com"]}
}

print(f"  ç™½åå•åŸŸåæ•°: {sum(len(v['domains']) for v in WHITELIST.values())}")

# === æ­¥éª¤ 3: æ•°æ®æ¨¡å‹ ===
print("\n[3/7] å®šä¹‰æ•°æ®æ¨¡å‹...")

class SourceInfo(BaseModel):
    url: str
    domain: str
    tier: str
    whitelisted: bool

class RawArticle(BaseModel):
    article_id: str
    url: str
    title: str
    snippet: str
    full_text: str = ""
    source: SourceInfo

print("  âœ“ æ¨¡å‹å®šä¹‰å®Œæˆ")

# === æ­¥éª¤ 4: å·¥å…·å‡½æ•° ===
print("\n[4/7] å®šä¹‰å·¥å…·å‡½æ•°...")

def resolve_source(url: str) -> SourceInfo:
    try:
        domain = url.split("/")[2].replace("www.", "")
    except:
        domain = "unknown"
    
    tier = "unknown"
    whitelisted = False
    
    for t, cfg in WHITELIST.items():
        if any(d in domain for d in cfg["domains"]):
            tier = t
            whitelisted = True
            break
    
    return SourceInfo(url=url, domain=domain, tier=tier, whitelisted=whitelisted)

print("  âœ“ resolve_source")

# === æ­¥éª¤ 5: æœç´¢å‡½æ•° ===
print("\n[5/7] å®šä¹‰æœç´¢å‡½æ•°...")

def search_simple(query: str, max_results: int = 20) -> List[Dict]:
    """ç®€åŒ–ç‰ˆæœç´¢"""
    print(f"\n  ğŸ” å¼€å§‹æœç´¢: {query}")
    print(f"      å‚æ•°: max_results={max_results}")
    
    try:
        print("      åˆ›å»º DDGS å®ä¾‹...")
        ddgs = DDGS()
        print("      âœ“ DDGS å®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        print("      è°ƒç”¨ ddgs.text()...")
        results = ddgs.text(keywords=query, max_results=max_results)
        print(f"      âœ“ ddgs.text() è¿”å›: {type(results)}")
        
        if results is None:
            print("      âš  è¿”å›å€¼ä¸º None")
            return []
        
        print("      è½¬æ¢ä¸ºåˆ—è¡¨...")
        results_list = list(results)
        print(f"      âœ“ è·å¾— {len(results_list)} æ¡ç»“æœ")
        
        return results_list
        
    except Exception as e:
        print(f"      âœ— æœç´¢å¼‚å¸¸: {type(e).__name__}: {str(e)[:100]}")
        import traceback
        traceback.print_exc()
        return []

print("  âœ“ search_simple")

# === æ­¥éª¤ 6: ä¸»é‡‡é›†å‡½æ•° ===
print("\n[6/7] å®šä¹‰ä¸»é‡‡é›†å‡½æ•°...")

def gather(queries: List[str], max_per_query: int = 2) -> List[RawArticle]:
    """ä¸»é‡‡é›†"""
    print(f"\nğŸ“Š å¼€å§‹é‡‡é›†:")
    print(f"   æŸ¥è¯¢æ•°: {len(queries)}")
    print(f"   æ¯æŸ¥è¯¢æœ€å¤š: {max_per_query} æ¡")
    
    articles = []
    
    for idx, query in enumerate(queries, 1):
        print(f"\n{'='*60}")
        print(f"[{idx}/{len(queries)}] å¤„ç†æŸ¥è¯¢: {query}")
        print('='*60)
        
        results = search_simple(query, max_results=30)
        
        if not results:
            print(f"  âœ— æ— ç»“æœï¼Œè·³è¿‡")
            continue
        
        print(f"\n  ğŸ“Š åŸå§‹ç»“æœ: {len(results)} æ¡")
        print(f"  å¼€å§‹è¿‡æ»¤...")
        
        found = 0
        filtered = 0
        
        for i, item in enumerate(results, 1):
            url = item.get("href", "")
            title = item.get("title", "")
            
            if not url:
                print(f"    [{i}] è·³è¿‡: æ—  URL")
                continue
            
            source = resolve_source(url)
            
            if not source.whitelisted:
                filtered += 1
                print(f"    [{i}] è¿‡æ»¤: {source.domain}")
                continue
            
            # å»é‡
            if any(a.url == url for a in articles):
                print(f"    [{i}] è·³è¿‡: é‡å¤ URL")
                continue
            
            print(f"    [{i}] âœ“ å‘½ä¸­: {source.tier} - {source.domain}")
            print(f"         {title[:60]}...")
            
            article = RawArticle(
                article_id=hashlib.md5(url.encode()).hexdigest(),
                url=url,
                title=title,
                snippet=item.get("body", ""),
                full_text="",
                source=source
            )
            
            articles.append(article)
            found += 1
            
            if found >= max_per_query:
                print(f"\n  âœ“ è¾¾åˆ°ä¸Šé™ ({max_per_query} æ¡)ï¼Œåœæ­¢")
                break
        
        print(f"\n  ğŸ“ˆ ç»Ÿè®¡: å‘½ä¸­ {found}, è¿‡æ»¤ {filtered}")
    
    return articles

print("  âœ“ gather")

# === æ­¥éª¤ 7: æ‰§è¡Œæµ‹è¯• ===
print("\n[7/7] å¼€å§‹æµ‹è¯•...")
print("="*60)

test_queries = [
    "python programming",           # é€šç”¨æµ‹è¯•
    "machine learning tutorial"     # é€šç”¨æµ‹è¯•
]

print(f"\næµ‹è¯•æŸ¥è¯¢: {test_queries}")

try:
    print("\nè°ƒç”¨ gather()...")
    articles = gather(test_queries, max_per_query=2)
    
    print("\n" + "="*60)
    print(f"âœ… é‡‡é›†å®Œæˆ: å…± {len(articles)} æ¡ç»“æœ")
    print("="*60)
    
    if articles:
        for i, art in enumerate(articles, 1):
            print(f"\n[{i}] {art.title}")
            print(f"    æ¥æº: {art.source.domain} ({art.source.tier})")
            print(f"    URL: {art.url[:80]}...")
    else:
        print("\nâš  æ— å‘½ä¸­ç»“æœ")
        print("\nå¯èƒ½åŸå› :")
        print("1. æœç´¢ç»“æœä¸­æ²¡æœ‰ç™½åå•åŸŸå")
        print("2. ç™½åå•é…ç½®è¿‡äºä¸¥æ ¼")
        print("3. DuckDuckGo è¿”å›ç»“æœè´¨é‡é—®é¢˜")

except Exception as e:
    print(f"\nâŒ é‡‡é›†è¿‡ç¨‹å‡ºé”™:")
    print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
    print(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
    import traceback
    print("\nå®Œæ•´å †æ ˆ:")
    traceback.print_exc()

print("\n" + "="*60)
print("ğŸ ç¨‹åºç»“æŸ")
print("="*60)